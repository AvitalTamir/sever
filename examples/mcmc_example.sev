// Example: Bayesian Linear Regression with MCMC
// This example demonstrates using MCMC to infer parameters of a linear model

// Prior distributions for parameters
let alpha ~ normal(0, 10);    // Intercept
let beta ~ normal(0, 5);      // Slope  
let sigma ~ gamma(2, 1);      // Noise standard deviation

// Generate some synthetic data
let n_data = 50;
let x_data = linspace(-5, 5, n_data);

// True parameters (unknown in real problems)
let true_alpha = 2.0;
let true_beta = 1.5;
let true_sigma = 0.8;

// Generate noisy observations
let y_data = array(n_data);
for i in 0..n_data {
    y_data[i] = true_alpha + true_beta * x_data[i] + normal(0, true_sigma);
}

// Likelihood function
fn likelihood(alpha: f64, beta: f64, sigma: f64) -> f64 {
    let log_prob = 0.0;
    for i in 0..n_data {
        let mean = alpha + beta * x_data[i];
        log_prob += normal_log_prob(y_data[i], mean, sigma);
    }
    return log_prob;
}

// Posterior = Prior * Likelihood
fn log_posterior(alpha: f64, beta: f64, sigma: f64) -> f64 {
    // Prior contributions
    let prior_alpha = normal_log_prob(alpha, 0, 10);
    let prior_beta = normal_log_prob(beta, 0, 5);
    let prior_sigma = gamma_log_prob(sigma, 2, 1);
    
    // Return log posterior
    return prior_alpha + prior_beta + prior_sigma + likelihood(alpha, beta, sigma);
}

// Run MCMC inference
let sampler = MCMCSampler {
    method: "adaptive_metropolis",
    num_samples: 5000,
    burnin: 1000,
    thin: 2,
    step_size: 0.1,
    adapt_step_size: true,
};

// Set parameter bounds
sampler.set_bounds("sigma", lower=0.01, upper=10.0);

// Initialize parameters
sampler.init_param("alpha", 0.0);
sampler.init_param("beta", 0.0);
sampler.init_param("sigma", 1.0);

// Run sampling
sampler.sample(log_posterior);

// Get results
let alpha_samples = sampler.get_samples("alpha");
let beta_samples = sampler.get_samples("beta");
let sigma_samples = sampler.get_samples("sigma");

// Print summary statistics
print("Parameter estimates (mean ± std):");
print("  alpha: {:.3f} ± {:.3f} (true: {:.3f})", 
      mean(alpha_samples), std(alpha_samples), true_alpha);
print("  beta:  {:.3f} ± {:.3f} (true: {:.3f})", 
      mean(beta_samples), std(beta_samples), true_beta);
print("  sigma: {:.3f} ± {:.3f} (true: {:.3f})", 
      mean(sigma_samples), std(sigma_samples), true_sigma);

// Check convergence
print("\nConvergence diagnostics:");
print("  Acceptance rate: {:.2%}", sampler.acceptance_rate());
print("  Effective sample size (alpha): {:.0f}", sampler.ess("alpha"));
print("  Effective sample size (beta):  {:.0f}", sampler.ess("beta"));
print("  Effective sample size (sigma): {:.0f}", sampler.ess("sigma"));

// Export results for further analysis
sampler.export_csv("mcmc_trace.csv");

// Posterior predictive checks
fn predict(x: f64, alpha: f64, beta: f64, sigma: f64) -> f64 {
    return normal(alpha + beta * x, sigma);
}

// Generate predictions at new points
let x_test = linspace(-6, 6, 100);
let y_pred_mean = array(100);
let y_pred_lower = array(100);
let y_pred_upper = array(100);

for i in 0..100 {
    let predictions = array(n_samples);
    for j in 0..n_samples {
        predictions[j] = predict(x_test[i], alpha_samples[j], beta_samples[j], sigma_samples[j]);
    }
    y_pred_mean[i] = mean(predictions);
    y_pred_lower[i] = quantile(predictions, 0.025);
    y_pred_upper[i] = quantile(predictions, 0.975);
}

print("\nPosterior predictive coverage at x=0: [{:.3f}, {:.3f}]", 
      y_pred_lower[50], y_pred_upper[50]);